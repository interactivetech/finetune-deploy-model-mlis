{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db132dfe",
   "metadata": {},
   "source": [
    "# Training and Deploying Gen AI at Scale with PCAI\n",
    "\n",
    "This notebook demonstrates a complete workflow for fine-tuning a pre-trained large language model, uploading the fine-tuned model to Hugging Face, and preparing it for deployment using our Machine Learning Inference Service. Fine-tuning is especially powerful for teaching new skills, tone, and formatting to a language model, thereby tailoring it to specialized tasks and business needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f1f5a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "1. **Model & Tokenizer Setup:** Load the pre-trained model and its tokenizer.\n",
    "2. **Dataset Preparation:** Create a synthetic dataset with a questionanswer example.\n",
    "3. **Text Generation Before Finetuning:** Generate baseline text from the model.\n",
    "4. **Fine-Tuning:** Train the model on the dataset to imbue it with new skills and tone.\n",
    "5. **Text Generation After Finetuning:** Evaluate improvements in the generated text.\n",
    "6. **Upload to Hugging Face Hub:** Share the fine-tuned model for deployment.\n",
    "7. **Deployment Overview:** Brief discussion on deploying the model via our inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2c389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# This notebook demonstrates fine-tuning a model to teach it new skills and tone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e217634",
   "metadata": {},
   "source": [
    "### Step 1: Model and Tokenizer Setup\n",
    "\n",
    "We begin by loading a pre-trained model (facebook/opt-125m) and its corresponding tokenizer. This model is chosen for demonstration purposes. Once loaded, the model is moved to GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a3df4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=False, verbose=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddcab50",
   "metadata": {},
   "source": [
    "### Step 2: Dataset Preparation\n",
    "\n",
    "Next, we create a synthetic dataset containing a single examplea question and answer pair that explains why an orange is orange. This example serves as a training signal to help the model learn a new tone and formatting style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27483d91-81a7-438b-a47f-8d0792fcf0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 9846\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 518\n",
      "    })\n",
      "})\n",
      "{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('timdettmers/openassistant-guanaco')\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b43a2bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Map: 100%|| 9846/9846 [00:32<00:00, 301.37 examples/s]\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Map: 100%|| 518/518 [00:02<00:00, 238.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a synthetic dataset with one example\n",
    "synthetic_text = (\n",
    "    \"Question: Why is an orange orange? Answer: The reason why an orange is orange is due to the presence of pigments \"\n",
    "    \"called carotenoids, specifically beta-carotene and other xanthophylls. These pigments are responsible for the orange, \"\n",
    "    \"yellow, and red colors of many fruits and vegetables.\"\n",
    ")\n",
    "# data_dict = {\"text\": [synthetic_text]}\n",
    "# dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# # Tokenization function for causal language modeling\n",
    "# def tokenize_function(examples):\n",
    "#     tokenized = tokenizer(\n",
    "#         examples[\"text\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=256,\n",
    "#     )\n",
    "#     # Use the input_ids as labels\n",
    "#     tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "#     return tokenized\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator to handle padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ab8db",
   "metadata": {},
   "source": [
    "### Step 3: Generating Text Before Finetuning\n",
    "\n",
    "Before fine-tuning, we generate text from the model to establish a baseline. This output will later be compared against the models performance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d61ec8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text before training:\n",
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate text before training\n",
    "print(\"\\nGenerated text before training:\")\n",
    "input_text = '''### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant:'''\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "output = model.generate(input_ids, max_length=256)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fdfba",
   "metadata": {},
   "source": [
    "### Step 4: Fine-Tuning the Model\n",
    "\n",
    "Using the Hugging Face `Trainer`, we fine-tune the model on our synthetic dataset. Fine-tuning is a key process for teaching the model new skills, tone, and formatting. Here, we configure the training arguments and initiate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a989ff30-db82-4b2d-8b91-e3339e6e12e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9846\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 518\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f0bd274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_244552/3120664308.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 01:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.615628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.620448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.645681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.663076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.689442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.700559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.698966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.728597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.745808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.755024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.769451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.781857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.788748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.795808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.794081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.799809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.800925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.797873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.797027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=0.13379300832748414, metrics={'train_runtime': 77.9541, 'train_samples_per_second': 32.84, 'train_steps_per_second': 4.105, 'total_flos': 334453800960000.0, 'train_loss': 0.13379300832748414, 'epoch': 20.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments and initialize the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./opt-125m-synthetic-finetuned2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'].select(range(128)),\n",
    "    eval_dataset=tokenized_dataset['test'].select(range(128)),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5e256",
   "metadata": {},
   "source": [
    "### Step 5: Generating Text After Finetuning\n",
    "\n",
    "After training, we generate text again. The changes in the output should reflect the new skills, tone, and formatting that the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88f00179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text after training:\n",
      "###Human: What is your name?###Assistant: My name is Assistant, and I am a person who has a vague interest in music. I have been listening to music for a while now, and I am enjoying it. However, I have a vague interest in music history and the current state of music. I would like to learn about the history of music and its impact on the world.### Human: Can you give me some examples of music that has been popular in the past year and a half?### Assistant: As a person who has a vague interest in music history and the current state of music, I can tell you some examples that have been popular in the past year and a half:\n",
      "\n",
      "1. Americana: This music is often used as a metaphor for change and progress. It is believed that this music was popular because of the changes made to the music industry and the changes in society as a result.\n",
      "\n",
      "2. Christian music: This is a popular music style that is popular across the board. It is believed that this music was popular because of the Christian faith.\n",
      "\n",
      "3. Americana: This music is often used as a metaphor for change and progress. It is believed that this music was popular because of the Christian faith\n"
     ]
    }
   ],
   "source": [
    "# Generate text after training\n",
    "print(\"\\nGenerated text after training:\")\n",
    "input_ids = tokenizer('###Human: What is your name?###Assistant:', return_tensors=\"pt\").input_ids.to(device)\n",
    "output = model.generate(input_ids, max_length=256)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eab380",
   "metadata": {},
   "source": [
    "### Step 6: Uploading the Finetuned Model to Hugging Face Hub\n",
    "\n",
    "Once fine-tuned, the model is uploaded to the Hugging Face Hub. This integration allows the model to be deployed at scale using our Machine Learning Inference Service, ensuring seamless production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "806b9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token:  路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token stored successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mendeza/opt-125m-synthetic-finetuned/commit/36a9c2f3424029c8a9b308cd65e5912627e0a957', commit_message='mendeza/opt-125m-finetuned', commit_description='', oid='36a9c2f3424029c8a9b308cd65e5912627e0a957', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mendeza/opt-125m-synthetic-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='mendeza/opt-125m-synthetic-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Prompt for Hugging Face token if not already set\n",
    "from getpass import getpass\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    hf_token = getpass(\"Enter your Hugging Face token: \")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "print(\"Hugging Face token stored successfully!\")\n",
    "\n",
    "# Login and push the fine-tuned model to the Hub\n",
    "login(token=hf_token)\n",
    "trainer.push_to_hub(\"mendeza/opt-125m-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60f66c67-c576-4f17-b661-2c0962e84350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mendeza/opt-125m-synthetic-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a92ef76-4be4-4497-99aa-d92fad267023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='mendeza/opt-125m-synthetic-finetuned', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926d665",
   "metadata": {},
   "source": [
    "### Step 7: Deploying via Machine Learning Inference Service\n",
    "\n",
    "With the model now hosted on Hugging Face, it can be easily integrated with our Machine Learning Inference Service to provide scalable and efficient predictions. The deployment specifics will depend on your production environment, but this workflow lays the groundwork for seamless integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "162cad0a-451a-443b-ba66-fae7b9ad98eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion result:  Organizing the orange includes several properties. The three-units, exact opposers, or individual phytosans selected for each pick of gray and yellow fruits and vegetables are responsible for the orange, yellow, and red colors of many fruits and vegetables. Each choice of ambi strutConnector moderation mixes functional foods with cooking functionality, and allow charity-appdeveloped innovative design of the recommendations. targeted and automated casual burasses and soap for the squashed, scruffy, and excepted appearance of pigments removed from specific forms of foodSaint skews (with or without no labels) the selection of flavors of fruits and vegetables. Social谢, \"Socialwin,\" or \"social elimination,\" refers to a concept wherein a plurality of equal correspondinglyilated community/parps/vegetation/verification, extraction, and/or counterv liabilities are chosen to help compensate for the importance of any meal the enemy and the enemy goal served, and are delivered to individual traveller groups in serving, pickifies, and/or cremains to meet the protective andreputed demands of the individual. preferences : The individual Gregg Social Neighborcknowledge offers an account of who the peasant selects by bespoke preferences \"for the preference of joey,\" at the origin, or takeover, of a and botany group. Freshly picked and snipped pigments are restricted to reach destinations where lead upwardly to the sweet vague, which is Riot fed by sedol, or the \"chicken,\" which is composed of progoi, labipay, and precisely defined adipodant gases. The health and367rd\n",
      "\n",
      "Questions: Why is an orange orange? Answer: The reason why an orange is orange is due to the presence of pigments called carotenoids, specifically beta-carotene and other xanthophylls. These pigments are responsible for the orange, yellow, and red colors of several fruits and vegetables.amuraic TheNitromeFan Weather Daily forced guiActive Fenum, Ac, C, and/or Contest Targets Team Hunting & Ranging- Rotational Recruiting- Vigilance of Meat andmeat to Collect Firearms for the Recruiting of Remunerative Loads by the Wearables &/or Weapons Committee of the ASA-CA, Secret Service, and/or Homeland Security Corps Joint List for Personnel and Civil Safety Inspector, Public Safety, and Financing Toelow searches for both enterices of the labor applicant for who is receiving the parties offer of a contraband or other lignime for their respective duties from any other Party, considered by the Section to have received the list from the registrar/entrar for who is placing an order for a specific farm or batch of farm/batch thereof, and whose identity is indicated by the yellow 'd',\" bc/HOO's\") FIRE Advisory, Minespline, and/or Extension Engagement Gamble, the understanders of pre- and post- Fallen and Tolling Emergency Red Flooring and Door Pits, and the Executives of Planters with Remote or Remote, Remotely Viable Forest Fire places, and the Tenders of RecruIT scouting & planning based on insider objections by Chisanadian, Dominican, and (Appl.) Litantikels, the Team Guides for Bowling, and Pitfalling Fire treacherous spaces, and agricultural heat conveying \"cessarosition\" or \"knub boat\" to the anciently worked fire of a sausage fire, or evaporated lava function. Multi-permission and everyone-access Trips to Fire tenders whose domains are responsible for the sight of which security crack or exits are desirable are such combined sweep reconnaissance swiping workmens' teams acting to cover the entire ability of each planer to keep locating the locations of often closed/open keys, by adding restraints in the shape of valvular openings (Internal, Open, and Mechanical) to a schedule of individual entry interests linked to the respective parametrit茅s of the specific initiator of the applications. They are nominated/guested by members of the Mateos, arranges teams, and participates by selecting and choosing the most appropriate and convenient use for each chosen location. They are given the right to individually enter/contacts the specific geographical contact for their proposed appointment, where they will be given time to develop/address the applicant's plan and present their consultative choice. They are ultimately recognized as certified as partyognists by the association participating in their biannual symposium/meetings. They collaborate as co-ordinator by choosing the least restrictive access arrangement and accepting their party invited representative or multiple Teamses per agreement. They are given #kg of declarations of office above representative and/or multiple Committee Condemns voiced objections to their preference for similar paths to the security/fire open area no later than seven days after declaration of office by member of the team being assigned the [idea]. They are given the right to reallocate remaining CLP time to their preferred candidate select lacking\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set the API key (can be an empty string for local vLLM) and base URL\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://127.0.0.1:8000/v1\"\n",
    "\n",
    "# Create an OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# Example: Create a completion\n",
    "completion = client.completions.create(\n",
    "    model=\"finetune_llm_with_lora/opt-125m-synthetic-finetuned/\", # Replace with your model if needed\n",
    "    prompt=\"Question: Why is an orange orange? Answer:\",\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(\"Completion result:\", completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57df483-a89b-4e3e-8cd7-2006ef920ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a21425-d2aa-4020-a4e7-9e98132d5193",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X 'POST' \\\n",
    "  'http://fb125m-harrow.default.mlds-kserve.us.rdlabs.hpecorp.net/v1/generate' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "   -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDk3NTc2NDIsImlhdCI6MTczOTM5MzI0MywiaXNzIjoiYWlvbGlAaHBlLmNvbSIsInN1YiI6IjFmZTBmZGVhLWZmMGUtNDExZS1hZTNmLTQ2ODQ3ZDA4Zjk0NSIsInVzZXIiOiJhZG1pbiJ9.yk0bm0qTvDMtGmy5Qv64wy0VYN7H3PtD44r5qmDmwEtVe24rVZcFTrv4DAtYI1Thu8qzd7OwVcs5YIUZ51mgkWuD6FYM8d2flSFYl9U_dP9-nVsTmn5DmMHt5PD--FVhgML2DJA4WNhtIbX-ocWPar-BJnJEZDHYVj4rtfHpwiWDv9YOEfLn9beoSoj_iIn6rdD5c0LZxcd1VMK6oG_gw_mZvRkARdQ4zwaGoMYXToLcMchXYKovP1BKmqga9ncBV4IU0Vt3yL6p0w6gap7cW2Eyg1XfVSzOS7L2DbEOdjJJcnzWsPdWWZ1D75HzpAxsI_ApWHDOQTjmL0xmCDle5g'\\\n",
    "  -d '{\n",
    "  \"prompt\": \"Question: Why is an orange orange? Answer:\",\n",
    "  \"stop\": [],\n",
    "  \"llm_config\": {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"min_length\": 0,\n",
    "    \"early_stopping\": false,\n",
    "    \"num_beams\": 1,\n",
    "    \"num_beam_groups\": 1,\n",
    "    \"use_cache\": true,\n",
    "    \"temperature\": 0.75,\n",
    "    \"top_k\": 15,\n",
    "    \"top_p\": 0.78,\n",
    "    \"typical_p\": 1,\n",
    "    \"epsilon_cutoff\": 0,\n",
    "    \"eta_cutoff\": 0,\n",
    "    \"diversity_penalty\": 0,\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"encoder_repetition_penalty\": 1,\n",
    "    \"length_penalty\": 1,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"renormalize_logits\": false,\n",
    "    \"remove_invalid_values\": false,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"output_attentions\": false,\n",
    "    \"output_hidden_states\": false,\n",
    "    \"output_scores\": false,\n",
    "    \"encoder_no_repeat_ngram_size\": 0,\n",
    "    \"n\": 1,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"use_beam_search\": false,\n",
    "    \"ignore_eos\": false,\n",
    "    \"skip_special_tokens\": true\n",
    "  },\n",
    "  \"adapter_name\": null\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2623471",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we demonstrated how to:\n",
    "\n",
    "- Fine-tune a pre-trained model to teach it new skills, tone, and formatting\n",
    "- Generate text before and after training to compare performance\n",
    "- Upload the fine-tuned model to the Hugging Face Hub\n",
    "  \n",
    "This process is key to deploying custom AI models at scale with PCAI, ensuring they are tailored to your specific needs and production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
