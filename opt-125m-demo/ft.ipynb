{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db132dfe",
   "metadata": {},
   "source": [
    "# Training and Deploying Gen AI at Scale with PCAI\n",
    "\n",
    "This notebook demonstrates a complete workflow for fine-tuning a pre-trained large language model, uploading the fine-tuned model to Hugging Face, and preparing it for deployment using our Machine Learning Inference Service. Fine-tuning is especially powerful for teaching new skills, tone, and formatting to a language model, thereby tailoring it to specialized tasks and business needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f1f5a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "1. **Model & Tokenizer Setup:** Load the pre-trained model and its tokenizer.\n",
    "2. **Dataset Preparation:** Create a synthetic dataset with a question–answer example.\n",
    "3. **Text Generation Before Finetuning:** Generate baseline text from the model.\n",
    "4. **Fine-Tuning:** Train the model on the dataset to imbue it with new skills and tone.\n",
    "5. **Text Generation After Finetuning:** Evaluate improvements in the generated text.\n",
    "6. **Upload to Hugging Face Hub:** Share the fine-tuned model for deployment.\n",
    "7. **Deployment Overview:** Brief discussion on deploying the model via our inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2c389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# This notebook demonstrates fine-tuning a model to teach it new skills and tone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e217634",
   "metadata": {},
   "source": [
    "### Step 1: Model and Tokenizer Setup\n",
    "\n",
    "We begin by loading a pre-trained model (facebook/opt-125m) and its corresponding tokenizer. This model is chosen for demonstration purposes. Once loaded, the model is moved to GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a3df4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=False, verbose=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddcab50",
   "metadata": {},
   "source": [
    "### Step 2: Dataset Preparation\n",
    "\n",
    "Next, we create a synthetic dataset containing a single example—a question and answer pair that explains why an orange is orange. This example serves as a training signal to help the model learn a new tone and formatting style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43a2bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 184.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a synthetic dataset with one example\n",
    "synthetic_text = (\n",
    "    \"Question: Why is an orange orange? Answer: The reason why an orange is orange is due to the presence of pigments \"\n",
    "    \"called carotenoids, specifically beta-carotene and other xanthophylls. These pigments are responsible for the orange, \"\n",
    "    \"yellow, and red colors of many fruits and vegetables.\"\n",
    ")\n",
    "data_dict = {\"text\": [synthetic_text]}\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Tokenization function for causal language modeling\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "    # Use the input_ids as labels\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator to handle padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ab8db",
   "metadata": {},
   "source": [
    "### Step 3: Generating Text Before Finetuning\n",
    "\n",
    "Before fine-tuning, we generate text from the model to establish a baseline. This output will later be compared against the model’s performance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d61ec8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text before training:\n",
      "Question: Why is an orange orange? Answer: Because it's a color.\n",
      "I think\n"
     ]
    }
   ],
   "source": [
    "# Generate text before training\n",
    "print(\"\\nGenerated text before training:\")\n",
    "input_text = \"Question: Why is an orange orange? Answer:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "output = model.generate(input_ids, max_length=20)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fdfba",
   "metadata": {},
   "source": [
    "### Step 4: Fine-Tuning the Model\n",
    "\n",
    "Using the Hugging Face `Trainer`, we fine-tune the model on our synthetic dataset. Fine-tuning is a key process for teaching the model new skills, tone, and formatting. Here, we configure the training arguments and initiate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0bd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments and initialize the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./opt-125m-synthetic-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5e256",
   "metadata": {},
   "source": [
    "### Step 5: Generating Text After Finetuning\n",
    "\n",
    "After training, we generate text again. The cell below is example code on how to run predictions on the finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe86934-da25-4682-b923-63dd6322477f",
   "metadata": {},
   "source": [
    "```python\n",
    "# Generate text after training\n",
    "print(\"\\nGenerated text after training:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "output = model.generate(input_ids, max_length=256)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eab380",
   "metadata": {},
   "source": [
    "### Step 6: Uploading the Finetuned Model to Hugging Face Hub\n",
    "\n",
    "Once fine-tuned, the model is uploaded to the Hugging Face Hub. \n",
    "This code wont run, but it is example code how to interactively add HF token to your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00d510-57c9-4ae3-8b7d-50bb33ceed28",
   "metadata": {},
   "source": [
    "```python\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Prompt for Hugging Face token if not already set\n",
    "from getpass import getpass\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    hf_token = getpass(\"Enter your Hugging Face token: \")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "print(\"Hugging Face token stored successfully!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480f211-9ffb-4cc0-8e74-6dfbc9f66f20",
   "metadata": {},
   "source": [
    "```python\n",
    "# Code to login and push fine-tuned model to the Hub\n",
    "login(token=hf_token)\n",
    "trainer.push_to_hub(\"mendeza/opt-125m-finetuned\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926d665",
   "metadata": {},
   "source": [
    "### Step 7: Deploying via Machine Learning Inference Service\n",
    "\n",
    "With the model now hosted on Hugging Face, it can be easily integrated with our Machine Learning Inference Service to provide scalable and efficient predictions. The deployment specifics will depend on your production environment, but this workflow lays the groundwork for seamless integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d20d7-6e04-4f98-8933-8d5d3039f2f9",
   "metadata": {},
   "source": [
    "## Step 7.1 Go to Tokens on MLIS, select `finetuned-opt-125m` and copy the Token created\n",
    "run cell below to enter Token interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66724ea4-7e33-4a5c-b60a-d9fc605262cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token stored successfully!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Prompt for Hugging Face token if not already set\n",
    "from getpass import getpass\n",
    "token = os.environ.get(\"token\")\n",
    "\n",
    "token = getpass(\"Enter your token: \")\n",
    "os.environ[\"token\"] = token\n",
    "\n",
    "print(\"Token stored successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ce972b-b040-4e69-8530-81196972b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion result:  The reason why an"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set API credentials\n",
    "openai_api_key = os.environ[\"token\"]\n",
    "openai_api_base = \"https://finetuned-opt-125m-predictor-admin-e51f23f1.saie02.tryezmeral.com/v1\"\n",
    "\n",
    "# Create an async OpenAI client\n",
    "client = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    "    timeout=60.0,\n",
    ")\n",
    "\n",
    "# Async function to stream completions\n",
    "async def stream_completion():\n",
    "    response = await client.completions.create(\n",
    "        model=\"mendeza/opt-125m-synthetic-finetuned\",\n",
    "        prompt=\"Question: Why is an orange orange? Answer:\",\n",
    "        max_tokens=4,\n",
    "        stream=True,  # Enable streaming,\n",
    "        top_p=1,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    print(\"Completion result:\", end=\" \", flush=True)\n",
    "    \n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].text:\n",
    "            print(chunk.choices[0].text, end=\"\", flush=True)  # Stream output\n",
    "\n",
    "# Run the async function properly in Jupyter\n",
    "await stream_completion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2623471",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we demonstrated how to:\n",
    "\n",
    "- Fine-tune a pre-trained model to teach it new skills, tone, and formatting\n",
    "- Generate text before and after training to compare performance\n",
    "- Upload the fine-tuned model to the Hugging Face Hub\n",
    "  \n",
    "This process is key to deploying custom AI models at scale with PCAI, ensuring they are tailored to your specific needs and production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
