{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4884c71-1b90-49d2-8108-2cc6bcdf583a",
   "metadata": {},
   "source": [
    "# End to End finetuning Llama 3.2 3B on custom chat dataset\n",
    "## using torchtune to finetune, vllm to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69529670-f225-4adf-87b5-b7cd730de689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 12 18:06:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             67W /  400W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98099fb2-3fa9-4088-b420-1f68eff5aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vllm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd7bf264-5ec4-431c-b87c-86f5bcd6b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchtune torchao bitsandbytes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83108e44-4ead-4a11-98cd-5c29804f4450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "Torchvision version: 0.20.1+cu124\n",
      "Torchao version: 0.8.0\n",
      "Bitsandbytes version: 0.45.2\n"
     ]
    }
   ],
   "source": [
    "# prompt: write code to print the versions of torch, torchvision, torchao, and bitsandbytes.\n",
    "\n",
    "# !pip install torch tune torchao bitsandbytes\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchao\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"Torchao version:\", torchao.__version__)\n",
    "print(\"Bitsandbytes version:\", bnb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c4c1d22-f409-4a8a-9bef-69f36158c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 1081.44 GB\n",
      "CPU Architecture: x86_64\n",
      "GPU Available: True\n",
      "GPU Name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {ram.total / 1e9:.2f} GB\")\n",
    "\n",
    "import platform\n",
    "\n",
    "print(f\"CPU Architecture: {platform.machine()}\")\n",
    "\n",
    "import torch\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"GPU Available: {gpu_available}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e46db32a-bf81-42c2-a7fb-a3461e013f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 676.17ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4130"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
    "ds2 = load_dataset('Open-Orca/SlimOrca-Dedup')\n",
    "ds2['train'].select(range(1)).to_json(\"chat_dataset_SlimOrca-Dedup2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18076276-5819-4ca6-ad82-b79af3e71f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Write an article based on this \"A man has been charged with murder and attempted murder after a woman and the man she was on a date with were stabbed at a restaurant in Sydney, Australia.\"'},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Title: Tragedy Strikes in Sydney: Victims Stabbed during Dinner Date, Suspect Charged with Murder and Attempted Murder\\n\\nIntroduction\\n\\nA seemingly ordinary evening took a tragic turn in Sydney, Australia when a romantic dinner date at a local restaurant ended in bloodshed, as a man allegedly attacked a couple, stabbing both victims. Local authorities have charged the suspect with murder and attempted murder in connection with the horrific crime. In this article, we delve into the chilling incident, the suspect, and the impact it has had on the community.\\n\\nThe Incident\\n\\nThe alarming event took place at a popular restaurant in Sydney, Australia. A woman and her date were enjoying their meal – blissfully unaware of the tragic circumstances that would soon unfold. As they shared conversation and indulged in their dinner, a man approached the couple and, without warning, began to stab them with a sharp object.\\n\\nPatrons at the restaurant scrambled for safety, as the assailant continued his violent rampage. Bystanders quickly called emergency services, who promptly arrived at the scene. Although both victims were rushed to a nearby hospital, the woman succumbed to her injuries. The man she was on a date with remained in critical condition, fighting for his life.\\n\\nThe Suspect\\n\\nInitial investigations revealed that the alleged attacker had no known connections to the victims – adding to the mystifying nature of this sudden and brutal assault. Witnesses reported that the assailant seemed to have no specific motive and appeared to carry out the act senselessly.\\n\\nFollowing a thorough investigation, local police identified and arrested the suspect. During the process, it was discovered that the alleged attacker had a history of criminal behavior and a troubled past, though it is unclear if this played a role in the tragic incident.\\n\\nAuthorities have formally charged the man with murder and attempted murder in connection with the heinous crime. He awaits a hearing to determine a trial date and, if convicted, could face a life sentence in prison.\\n\\nThe Community's Response\\n\\nThe shocking nature of the crime has left the residents of Sydney reeling, as they struggle to come to terms with the harrowing event. The restaurant where the attack occurred has since been closed, with a makeshift memorial being created outside to commemorate the victims.\\n\\nMany have questioned how such a vicious crime could happen in what is considered to be one of the safest cities in the world. This tragic event has spurred local officials to reassess current security measures and devise strategies to reinforce public safety. Additionally, discussions surrounding mental health and criminal rehabilitation have surfaced as residents seek to comprehend the actions of the alleged perpetrator and prevent similar incidents from occurring in the future.\\n\\nIn the wake of the stabbing, the community has banded together with an outpouring of grief and support for the victims and their families. Candlelight vigils have been held, and an online fundraising campaign is underway to assist the surviving victim with his medical expenses and recovery.\\n\\nConclusion\\n\\nThe tragic attack in Sydney serves as a chilling reminder that senseless acts of violence can happen anywhere and at any time. The community's response to this horrific and seemingly random act of brutality has been one of solidarity and determination to prevent such incidents in the future. As the case unfolds, the victims and their families remain in the hearts of the community, who are grieving the devastating loss of a life cut tragically short and supporting the recovering victim as he continues to endure this unimaginable ordeal.\"}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Data \n",
    "ds2['train'].select(range(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462ccd1-a60a-43c2-91e2-156edba79a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef41254a-cd9c-43be-a7c6-2ee995d35092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/home/Llama-3.2-3B-Instruct/' is ready.\n"
     ]
    }
   ],
   "source": [
    "# create directory to save pretrained and finetuned model\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = \"/home/Llama-3.2-3B-Instruct/\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "print(f\"Directory '{directory_path}' is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b20bc4-df2a-408d-a9f9-69a8f8bb436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
      "\n",
      "Welcome to the torchtune CLI!\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "subcommands:\n",
      "  {download,ls,cp,run,validate}\n",
      "    download            Download a model from the Hugging Face Hub or Kaggle\n",
      "                        Model Hub.\n",
      "    ls                  List all built-in recipes and configs\n",
      "    cp                  Copy a built-in recipe or config to a local path.\n",
      "    run                 Run a recipe. For distributed recipes, this supports\n",
      "                        all torchrun arguments.\n",
      "    validate            Validate a config and ensure that it is well-formed.\n"
     ]
    }
   ],
   "source": [
    "!tune --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ed7a69-34b1-4a45-ab48-43215e1c2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model\n",
    "# !tune download meta-llama/Llama-3.2-3B-Instruct \\\n",
    "#   --output-dir /home/Llama-3.2-3B-Instruct/ \\\n",
    "#   --hf-token <HF_TOKRN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf5030-d0ca-41aa-878d-ead8db888821",
   "metadata": {},
   "source": [
    "# Finetune Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc8699d9-7a2a-414a-a295-263e183e60f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/finetune-cfg-cpu.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/finetune-cfg-cpu.yaml\n",
    "output_dir: /home/torchtune/llama3_2_3B/full_single_device_cpu # /tmp may be deleted by your system. Change it to your preference.\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer:\n",
    "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
    "  path: /home/Llama-3.2-3B-Instruct/original/tokenizer.model\n",
    "  max_seq_len: null\n",
    "\n",
    "# Dataset\n",
    "\n",
    "dataset:\n",
    "  _component_: torchtune.datasets.chat_dataset\n",
    "  source: json\n",
    "  conversation_column: conversations\n",
    "  conversation_style: sharegpt\n",
    "  data_files: /home/chat_dataset_SlimOrca-Dedup2.json\n",
    "  split: train\n",
    "\n",
    "# dataset:\n",
    "#   _component_: torchtune.datasets.alpaca_dataset\n",
    "#   packed: False  # True increases speed\n",
    "seed: null\n",
    "shuffle: True\n",
    "\n",
    "# Model Arguments\n",
    "model:\n",
    "  _component_: torchtune.models.llama3_2.llama3_2_3b\n",
    "\n",
    "checkpointer:\n",
    "  _component_: torchtune.training.FullModelHFCheckpointer\n",
    "  checkpoint_dir: /home/Llama-3.2-3B-Instruct/\n",
    "  checkpoint_files: [\n",
    "    model-00001-of-00002.safetensors,\n",
    "    model-00002-of-00002.safetensors,\n",
    "  ]\n",
    "  recipe_checkpoint: null\n",
    "  output_dir: ${output_dir}\n",
    "  model_type: LLAMA3_2\n",
    "resume_from_checkpoint: False\n",
    "\n",
    "# Fine-tuning arguments\n",
    "batch_size: 1\n",
    "epochs: 1\n",
    "optimizer:\n",
    "  _component_: bitsandbytes.optim.PagedAdamW8bit\n",
    "  lr: 2e-5\n",
    "loss:\n",
    "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
    "max_steps_per_epoch: null\n",
    "gradient_accumulation_steps: 1  # Use to increase effective batch size\n",
    "optimizer_in_bwd: True  # True saves memory. Requires gradient_accumulation_steps=1\n",
    "clip_grad_norm: null\n",
    "compile: False  # torch.compile the model + loss, True increases speed + decreases memory\n",
    "\n",
    "# Training environment\n",
    "device: cpu\n",
    "\n",
    "# Memory management\n",
    "enable_activation_checkpointing: True  # True reduces memory\n",
    "enable_activation_offloading: False  # True reduces memory\n",
    "\n",
    "# Reduced precision\n",
    "dtype: fp32\n",
    "\n",
    "# Logging\n",
    "metric_logger:\n",
    "  _component_: torchtune.training.metric_logging.DiskLogger\n",
    "  log_dir: ${output_dir}/logs\n",
    "log_every_n_steps: 1\n",
    "log_peak_memory_stats: True\n",
    "\n",
    "\n",
    "# Profiler (disabled)\n",
    "profiler:\n",
    "  _component_: torchtune.training.setup_torch_profiler\n",
    "  enabled: False\n",
    "\n",
    "  #Output directory of trace artifacts\n",
    "  output_dir: ${output_dir}/profiling_outputs\n",
    "\n",
    "  #`torch.profiler.ProfilerActivity` types to trace\n",
    "  cpu: True\n",
    "  cuda: True\n",
    "\n",
    "  #trace options passed to `torch.profiler.profile`\n",
    "  profile_memory: False\n",
    "  with_stack: False\n",
    "  record_shapes: True\n",
    "  with_flops: False\n",
    "\n",
    "  # `torch.profiler.schedule` options:\n",
    "  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat\n",
    "  wait_steps: 5\n",
    "  warmup_steps: 3\n",
    "  active_steps: 2\n",
    "  num_cycles: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95471807-e623-4110-a4be-f59c846d9f7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# To run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d763fb-0361-45bf-b1cb-db39d7b2396a",
   "metadata": {},
   "source": [
    "```bash\n",
    "%%writefile /home/finetune-cfg.yaml\n",
    "output_dir: /home/torchtune/llama3_2_3B/full_single_device # /tmp may be deleted by your system. Change it to your preference.\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer:\n",
    "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
    "  path: /home/Llama-3.2-3B-Instruct/original/tokenizer.model\n",
    "  max_seq_len: null\n",
    "\n",
    "# Dataset\n",
    "\n",
    "dataset:\n",
    "  _component_: torchtune.datasets.chat_dataset\n",
    "  source: json\n",
    "  conversation_column: conversations\n",
    "  conversation_style: sharegpt\n",
    "  data_files: /home/chat_dataset_SlimOrca-Dedup2.json\n",
    "  split: train\n",
    "\n",
    "# dataset:\n",
    "#   _component_: torchtune.datasets.alpaca_dataset\n",
    "#   packed: False  # True increases speed\n",
    "seed: null\n",
    "shuffle: True\n",
    "\n",
    "# Model Arguments\n",
    "model:\n",
    "  _component_: torchtune.models.llama3_2.llama3_2_3b\n",
    "\n",
    "checkpointer:\n",
    "  _component_: torchtune.training.FullModelHFCheckpointer\n",
    "  checkpoint_dir: /home/Llama-3.2-3B-Instruct/\n",
    "  checkpoint_files: [\n",
    "    model-00001-of-00002.safetensors,\n",
    "    model-00002-of-00002.safetensors,\n",
    "  ]\n",
    "  recipe_checkpoint: null\n",
    "  output_dir: ${output_dir}\n",
    "  model_type: LLAMA3_2\n",
    "resume_from_checkpoint: False\n",
    "\n",
    "# Fine-tuning arguments\n",
    "batch_size: 1\n",
    "epochs: 1\n",
    "optimizer:\n",
    "  _component_: bitsandbytes.optim.PagedAdamW8bit\n",
    "  lr: 2e-5\n",
    "loss:\n",
    "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
    "max_steps_per_epoch: null\n",
    "gradient_accumulation_steps: 1  # Use to increase effective batch size\n",
    "optimizer_in_bwd: True  # True saves memory. Requires gradient_accumulation_steps=1\n",
    "clip_grad_norm: null\n",
    "compile: False  # torch.compile the model + loss, True increases speed + decreases memory\n",
    "\n",
    "# Training environment\n",
    "device: cuda\n",
    "\n",
    "# Memory management\n",
    "enable_activation_checkpointing: True  # True reduces memory\n",
    "enable_activation_offloading: False  # True reduces memory\n",
    "\n",
    "# Reduced precision\n",
    "dtype: bf16\n",
    "\n",
    "# Logging\n",
    "metric_logger:\n",
    "  _component_: torchtune.training.metric_logging.DiskLogger\n",
    "  log_dir: ${output_dir}/logs\n",
    "log_every_n_steps: 1\n",
    "log_peak_memory_stats: True\n",
    "\n",
    "\n",
    "# Profiler (disabled)\n",
    "profiler:\n",
    "  _component_: torchtune.training.setup_torch_profiler\n",
    "  enabled: False\n",
    "\n",
    "  #Output directory of trace artifacts\n",
    "  output_dir: ${output_dir}/profiling_outputs\n",
    "\n",
    "  #`torch.profiler.ProfilerActivity` types to trace\n",
    "  cpu: True\n",
    "  cuda: True\n",
    "\n",
    "  #trace options passed to `torch.profiler.profile`\n",
    "  profile_memory: False\n",
    "  with_stack: False\n",
    "  record_shapes: True\n",
    "  with_flops: False\n",
    "\n",
    "  # `torch.profiler.schedule` options:\n",
    "  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat\n",
    "  wait_steps: 5\n",
    "  warmup_steps: 3\n",
    "  active_steps: 2\n",
    "  num_cycles: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f363a-16ef-40aa-ae8b-a6969ceec4b9",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4f1e684-3fc4-49d6-95f0-7155660b1c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/Llama-3.2-3B-Instruct/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - model-00002-of-00002.safetensors\n",
      "  model_type: LLAMA3_2\n",
      "  output_dir: /home/torchtune/llama3_2_3B/full_single_device_cpu\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.chat_dataset\n",
      "  conversation_column: conversations\n",
      "  conversation_style: sharegpt\n",
      "  data_files: /home/chat_dataset_SlimOrca-Dedup2.json\n",
      "  source: json\n",
      "  split: train\n",
      "device: cpu\n",
      "dtype: fp32\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/torchtune/llama3_2_3B/full_single_device_cpu/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_2.llama3_2_3b\n",
      "optimizer:\n",
      "  _component_: bitsandbytes.optim.PagedAdamW8bit\n",
      "  lr: 2.0e-05\n",
      "optimizer_in_bwd: true\n",
      "output_dir: /home/torchtune/llama3_2_3B/full_single_device_cpu\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /home/torchtune/llama3_2_3B/full_single_device_cpu/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /home/Llama-3.2-3B-Instruct/original/tokenizer.model\n",
      "\n",
      "INFO:torchtune.utils._logging:log_peak_memory_stats was set to True, however, training does not use cuda. Setting log_peak_memory_stats=False.\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 2919765306. Local seed is seed + rank = 2919765306 + 0\n",
      "Writing logs to /home/torchtune/llama3_2_3B/full_single_device_cpu/logs/log_1739308442.txt\n",
      "INFO:torchtune.utils._logging:Model is initialized with precision torch.float32.\n",
      "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
      "INFO:torchtune.utils._logging:In-backward optimizers are set up.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "INFO:torchtune.utils._logging:No learning rate scheduler configured. Using constant learning rate.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "^C0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torchtune/_cli/tune.py\", line 49, in main\n",
      "    parser.run(args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torchtune/_cli/tune.py\", line 43, in run\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torchtune/_cli/run.py\", line 214, in _run_cmd\n",
      "    self._run_single_device(args, is_builtin=is_builtin)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torchtune/_cli/run.py\", line 108, in _run_single_device\n",
      "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 289, in run_path\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 96, in _run_module_code\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/recipes/full_finetune_single_device.py\", line 803, in <module>\n",
      "    sys.exit(recipe_main())\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "    sys.exit(recipe_main(conf))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/recipes/full_finetune_single_device.py\", line 798, in recipe_main\n",
      "    recipe.train()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/recipes/full_finetune_single_device.py\", line 702, in train\n",
      "    current_loss = self._loss_step(batch) * current_num_tokens\n",
      "  File \"/usr/local/lib/python3.10/site-packages/recipes/full_finetune_single_device.py\", line 630, in _loss_step\n",
      "    logits = self._model(**batch)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torchtune/modules/transformer.py\", line 635, in forward\n",
      "    h = layer(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py\", line 170, in forward\n",
      "    return self.checkpoint_fn(  # type: ignore[misc]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 496, in checkpoint\n",
      "    ret = function(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torchtune/modules/transformer.py\", line 128, in forward\n",
      "    mlp_out = self.mlp(self.mlp_norm(h))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torchtune/modules/feed_forward.py\", line 50, in forward\n",
      "    h = self.activation(self.w1(x))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#cpu\n",
    "# finetune-cfg-cpu.yam\n",
    "!tune run full_finetune_single_device --config /home/finetune-cfg-cpu.yaml checkpointer.checkpoint_dir=/home/Llama-3.2-3B-Instruct/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd7f7b-7805-4e8f-a06d-550dfac612cc",
   "metadata": {},
   "source": [
    "```bash\n",
    "#GPU\n",
    "!tune run full_finetune_single_device --config /home/finetune-cfg.yaml checkpointer.checkpoint_dir=/home/Llama-3.2-3B-Instruct/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd3ec6-0f65-4896-ae58-2e57d3b1e5eb",
   "metadata": {},
   "source": [
    "# Test Generation of finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b37d567-320b-4f35-8f35-909b3b17b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree -a /home/torchtune/llama3_2_3B/full_single_device/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3daf39b-1640-44ad-8278-184389673595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/cfg_gen.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/cfg_gen.yaml\n",
    "output_dir: ./ # Not needed\n",
    "\n",
    "# Model arguments\n",
    "model:\n",
    "  _component_: torchtune.models.llama3_2.llama3_2_3b\n",
    "\n",
    "checkpointer:\n",
    "  _component_: torchtune.training.FullModelHFCheckpointer\n",
    "  checkpoint_dir: /home/torchtune/llama3_2_3B/full_single_device/epoch_0/\n",
    "  checkpoint_files: [\n",
    "    ft-model-00001-of-00002.safetensors,\n",
    "    ft-model-00002-of-00002.safetensors,\n",
    "  ]\n",
    "  output_dir: ${output_dir}\n",
    "  model_type: LLAMA3_2\n",
    "\n",
    "device: cuda\n",
    "dtype: bf16\n",
    "\n",
    "seed: 1234\n",
    "\n",
    "# Tokenizer arguments\n",
    "tokenizer:\n",
    "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
    "  path: /home/Llama-3.2-3B-Instruct/original/tokenizer.model\n",
    "  max_seq_len: null\n",
    "  prompt_template: null\n",
    "\n",
    "# Generation arguments; defaults taken from gpt-fast\n",
    "prompt:\n",
    "  system: null\n",
    "  user: \"Tell me a joke.\"\n",
    "max_new_tokens: 300\n",
    "temperature: 0.6 # 0.8 and 0.6 are popular values to try\n",
    "top_k: 300\n",
    "\n",
    "enable_kv_cache: True\n",
    "\n",
    "quantizer: null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0390de2e-ceca-4f98-9dba-f364351c2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run generate --config /home/cfg_gen.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca83f0-2858-43c7-89ed-806f30218382",
   "metadata": {},
   "source": [
    "# Upload to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ae2d31-929f-4e59-b46b-77a62561b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload mendeza/Llama-3.2-3B-Instruct /home/torchtune/llama3_2_3B/full_single_device/epoch_0/  --token <HF_TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cc05c-5c5b-4fd4-8403-93321df7c211",
   "metadata": {},
   "source": [
    "# Test loading and running with VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ca95da-b7b8-4a2f-9ebd-e5ce0565e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fdf9394-c75c-46cd-b2de-45c2acf5f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def print_outputs(outputs):\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "#TODO: update it to your chosen epoch\n",
    "llm = LLM(\n",
    "    model=\"/home/torchtune/llama3_2_3B/full_single_device/epoch_0/\",\n",
    "    load_format=\"safetensors\",\n",
    "    kv_cache_dtype=\"auto\",\n",
    "    max_model_len=4096\n",
    ")\n",
    "sampling_params = SamplingParams(max_tokens=16, temperature=0.5)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write an essay about the importance of higher education.\",\n",
    "    },\n",
    "]\n",
    "outputs = llm.chat(conversation, sampling_params=sampling_params, use_tqdm=False)\n",
    "print_outputs(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd1ecf-e288-4824-a401-5737f29d03cb",
   "metadata": {},
   "source": [
    "# Deploy VLLM Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf17c5-3877-4377-8de7-365bbfd33f41",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server --model /home/torchtune/llama3_2_3B/full_single_device/epoch_0/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ebac799-53e6-44b2-8bac-bea3abc50c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d835274-a343-4e0d-b7bf-4674c4e3d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set the API key (can be an empty string for local vLLM) and base URL\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://127.0.0.1:8000/v1\"\n",
    "\n",
    "# Create an OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# Example: Create a completion\n",
    "completion = client.completions.create(\n",
    "    model=\"/home/torchtune/llama3_2_3B/full_single_device/epoch_0/\", # Replace with your model if needed\n",
    "    prompt=\"How to implement binary cross entropy using torch primitves in python\",\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(\"Completion result:\", completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57c491e6-bca4-4bbf-bd74-40d9be389fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daed5f93-05e5-45db-a1ff-19de503337af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Apply nest_asyncio to avoid \"RuntimeError: asyncio.run()...\"\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set API credentials\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://127.0.0.1:8000/v1\"\n",
    "\n",
    "# Create an async OpenAI client\n",
    "client = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# Async function to stream completions\n",
    "async def stream_completion():\n",
    "    response = await client.completions.create(\n",
    "        model=\"/home/torchtune/llama3_2_3B/full_single_device/epoch_0/\",\n",
    "        prompt=\"How to implement binary cross entropy using torch primitives in Python\",\n",
    "        max_tokens=1024,\n",
    "        stream=True,  # Enable streaming,\n",
    "        top_p=1,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    print(\"Completion result:\", end=\" \", flush=True)\n",
    "    \n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].text:\n",
    "            print(chunk.choices[0].text, end=\"\", flush=True)  # Stream output\n",
    "\n",
    "# Run the async function properly in Jupyter\n",
    "await stream_completion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba423b-1d48-4623-adf0-80fa7e61a704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
